{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e3451b",
   "metadata": {},
   "source": [
    "# Assignment 3. Text mining using Spark streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46458f29",
   "metadata": {},
   "source": [
    "## Group 23 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa628b1",
   "metadata": {},
   "source": [
    "## Collecting data from Spark streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a219a865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport os\\nimport glob\\nfrom tqdm import tqdm'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the necessary libraries\n",
    "'''import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f79fd0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data = pd.DataFrame()\\nfolder_list = os.listdir(\"C:/Users/altyn/Desktop/spark/\")\\nfor folder in tqdm(folder_list):\\n    os.chdir(os.path.join(\"C:/Users/altyn/Desktop/spark/\", folder))\\n    files = glob.glob(\\'*\\'.format(\\'json\\'))\\n    files.pop(-1)\\n    for file in files:\\n        df = pd.read_json(file, lines=True)\\n        data=pd.concat([data, df])\\n\\ndata = data.reset_index()\\ndata = data.drop(\\'index\\', axis=1)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the collected data from multiple folders and merging into one data frame\n",
    "'''data = pd.DataFrame()\n",
    "folder_list = os.listdir(\"C:/Users/altyn/Desktop/spark/\")\n",
    "for folder in tqdm(folder_list):\n",
    "    os.chdir(os.path.join(\"C:/Users/altyn/Desktop/spark/\", folder))\n",
    "    files = glob.glob('*'.format('json'))\n",
    "    files.pop(-1)\n",
    "    for file in files:\n",
    "        df = pd.read_json(file, lines=True)\n",
    "        data=pd.concat([data, df])\n",
    "\n",
    "data = data.reset_index()\n",
    "data = data.drop('index', axis=1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa7ef26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"X_train['channel'] = y_train\\nX_test['channel'] = y_test\\n\\nX_train.to_csv('data/train.csv')\\nX_test.to_csv('data/test.csv')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting the data into training and testing samples.\n",
    "'''from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop('channel', axis=1)\n",
    "y = data['channel']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)'''\n",
    "\n",
    "#saving data to csv files\n",
    "'''X_train['channel'] = y_train\n",
    "X_test['channel'] = y_test\n",
    "\n",
    "X_train.to_csv('data/train.csv')\n",
    "X_test.to_csv('data/test.csv')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4160789e",
   "metadata": {},
   "source": [
    "Both files with training and testing samples were uploaded to github for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d598ed5",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d29a63f",
   "metadata": {},
   "source": [
    "As the dataset is large, we are going to use spark DataFrame type to accelerate the calculations. Our data has 4 columns:\n",
    "\n",
    " - datetime (the moment the message was sent to the chat)\n",
    "\n",
    " - username\n",
    "\n",
    " - message\n",
    "\n",
    " - channel\n",
    "\n",
    "Below we can see the first 20 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71770851",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+----------+\n",
      "|datetime|            username|             message|   channel|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "| 33:57.5|      thenormalbeast|             hahahah|#asmongold|\n",
      "| 04:12.0|            orckykat|amber slaps girls...|#asmongold|\n",
      "| 59:14.5|           intranett| HAHAHAHAHHAHAHAHAHA|#asmongold|\n",
      "| 28:37.0|               erixp|     BROLLAN????????|      #pgl|\n",
      "| 09:41.0|        cerensdipity|It’s cheaper to p...|#asmongold|\n",
      "| 14:53.8|               thied|                  no|#asmongold|\n",
      "| 29:54.4|          poggz__one|     somebody farted|#asmongold|\n",
      "| 19:45.8|iknowanythingabou...|               Obama|#asmongold|\n",
      "| 57:33.1|            madjelly|depp stole amber'...|#asmongold|\n",
      "| 54:56.3|            klagitsz|chr1st1an1337:  D...|      #pgl|\n",
      "| 41:36.0|      humanlawnmower|                KEKW|#asmongold|\n",
      "| 34:17.2|             setcrow|           MR WRIGHT|#asmongold|\n",
      "| 42:49.6|              voryen|All to Gaules str...|      #pgl|\n",
      "| 32:30.2|            wartoonz|DID IMPERIAL EVEN...|      #pgl|\n",
      "| 21:46.7|              aur1so|           PepeLaugh|#asmongold|\n",
      "| 43:46.8|           arvesroke|TheIlluminati The...|#asmongold|\n",
      "| 55:23.6|              fadaar|WHY WOULD JOHNNY ...|#asmongold|\n",
      "| 44:49.5|          sunfyre_08|                KEKW|#asmongold|\n",
      "| 35:10.8|        biggeordieuk|            Clueless|#asmongold|\n",
      "| 21:22.3|      cultofpcnality|I wonder if there...|#asmongold|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading the file and converting to spark DataFrame\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_raw = pd.read_csv(r\"data\\train.csv\", \\\n",
    "                        index_col=0).reset_index(drop=True)\n",
    "train_df = spark.createDataFrame(train_raw.astype(str))\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d1625",
   "metadata": {},
   "source": [
    "The column of interest is `message` which contains the text of the comments. We need to preprocess it and covert into features we can use for modelling. The main tool we are using is `pyspark.ml`. We faced some problems since it differs from the `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "312bf85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary classes from pyspark.ml for preprocessing and building the model\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionSummary\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer, CountVectorizer, Binarizer, StringIndexer, IndexToString\n",
    "from pyspark.sql.functions import lower, regexp_replace, col\n",
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c448a27",
   "metadata": {},
   "source": [
    "First of all, the text is cleaned out of the unnecessary unformation like numbers, symbols, usernames (when the message is a response to the previous message). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbbc1c83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+----------+--------------------+\n",
      "|datetime|      username|             message|   channel|       message_clean|\n",
      "+--------+--------------+--------------------+----------+--------------------+\n",
      "| 33:57.5|thenormalbeast|             hahahah|#asmongold|             hahahah|\n",
      "| 04:12.0|      orckykat|amber slaps girls...|#asmongold|amber slaps girls...|\n",
      "| 59:14.5|     intranett| HAHAHAHAHHAHAHAHAHA|#asmongold| hahahahahhahahahaha|\n",
      "| 28:37.0|         erixp|     BROLLAN????????|      #pgl|             brollan|\n",
      "| 09:41.0|  cerensdipity|It’s cheaper to p...|#asmongold|it’s cheaper to p...|\n",
      "+--------+--------------+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text(c):\n",
    "    c = lower(c)\n",
    "    c = regexp_replace(c, \"(https?\\://)\\S+\", \"\") # Remove links\n",
    "    c = regexp_replace(c, \"(\\\\n)|\\n|\\r|\\t\", \"\") # Remove CR, tab, and LR\n",
    "    c = regexp_replace(c, \"(?:(?:[0-9]{2}[:\\/,]){2}[0-9]{2,4})\", \"\") # Remove dates\n",
    "    c = regexp_replace(c, \"@([A-Za-z0-9_]+)\", \"\") # Remove usernames\n",
    "    c = regexp_replace(c, \"[0-9]\", \"\") # Remove numbers\n",
    "    c = regexp_replace(c, \"\\:|\\/|\\#|\\.|\\?|\\!|\\&|\\\"|\\,\", \"\") # Remove symbols\n",
    "    return c\n",
    "\n",
    "train_df = train_df.withColumn(\"message_clean\", clean_text(col(\"message\")))\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1fd612",
   "metadata": {},
   "source": [
    "From the first 5 rows, it is visible that the text is successfully cleaned.\n",
    "\n",
    "Now we can build a pipeline which transforms the text into valid features:\n",
    "\n",
    "1. encoding the target channel value into binary variable\n",
    "\n",
    "2. tokenizing the message into separate words\n",
    "\n",
    "3. removing the stop words which are meaningless\n",
    "\n",
    "4. transforming the words into vectors. We do not take words which are met less than 20 comments in the dataset, otherwise we end up with too many features.\n",
    "\n",
    "5. as we do not care how often the word is repeated in the message, we transform them into binary vectors\n",
    "\n",
    "6. finally building the logistic regression\n",
    "\n",
    "7. decoding the binary target variable into the names of the channels for prediction column\n",
    "\n",
    "8. building a pipeline for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501d8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of 6 stages:\n",
    "\n",
    "stringind = StringIndexer(inputCol=\"channel\", outputCol=\"channel_bin\")\n",
    "tokenizer = Tokenizer(inputCol=\"message_clean\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"words_clean\", stopWords= eng_stopwords)\n",
    "cv = CountVectorizer(inputCol=\"words_clean\", outputCol=\"features\", binary=True, minDF = 20)\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol = \"channel_bin\")\n",
    "indtostr = IndexToString(inputCol=\"prediction\", outputCol=\"pred_channel\", labels = ['#asmongold', '#pgl'])\n",
    "\n",
    "pipeline = Pipeline(stages=[stringind,tokenizer, remover,cv, lr, indtostr])\n",
    "\n",
    "#fitting the pipeline to training data set\n",
    "model = pipeline.fit(train_df)\n",
    "prediction = model.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b84e73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>festival</td>\n",
       "      <td>-33.223894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>sponsored</td>\n",
       "      <td>-23.564451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119</th>\n",
       "      <td>joining</td>\n",
       "      <td>-21.089192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>asmonweird</td>\n",
       "      <td>-20.886121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>gotcha</td>\n",
       "      <td>-20.674827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>mats</td>\n",
       "      <td>-20.362149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>bruise</td>\n",
       "      <td>-19.231872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>judge</td>\n",
       "      <td>-18.45312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>otk</td>\n",
       "      <td>-18.250787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>jd</td>\n",
       "      <td>-17.729267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3178</th>\n",
       "      <td>friendship</td>\n",
       "      <td>-17.380938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>ult</td>\n",
       "      <td>-17.317484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>poker</td>\n",
       "      <td>-16.763247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3205</th>\n",
       "      <td>list</td>\n",
       "      <td>-16.502862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>medical</td>\n",
       "      <td>-16.413605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>genshin</td>\n",
       "      <td>-16.225412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>shorter</td>\n",
       "      <td>-15.985886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>asmondad</td>\n",
       "      <td>-15.651961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>surgery</td>\n",
       "      <td>-15.346932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>donating</td>\n",
       "      <td>-14.770176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word     weight\n",
       "2943    festival -33.223894\n",
       "1552   sponsored -23.564451\n",
       "3119     joining -21.089192\n",
       "2683  asmonweird -20.886121\n",
       "2647      gotcha -20.674827\n",
       "2696        mats -20.362149\n",
       "2784      bruise -19.231872\n",
       "320        judge  -18.45312\n",
       "842          otk -18.250787\n",
       "746           jd -17.729267\n",
       "3178  friendship -17.380938\n",
       "2337         ult -17.317484\n",
       "890        poker -16.763247\n",
       "3205        list -16.502862\n",
       "1199     medical -16.413605\n",
       "2782     genshin -16.225412\n",
       "2991     shorter -15.985886\n",
       "1765    asmondad -15.651961\n",
       "2672     surgery -15.346932\n",
       "2466    donating -14.770176"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#analyzing the corfficients\n",
    "coeffs = pd.DataFrame([model.stages[3].vocabulary, model.stages[4].coefficients]).T\n",
    "coeffs.columns = ['word','weight']\n",
    "coeffs.sort_values(by='weight', ascending = True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2396e29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3103</th>\n",
       "      <td>library</td>\n",
       "      <td>87.844072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>xantares</td>\n",
       "      <td>43.479954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>37.353896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>numerous</td>\n",
       "      <td>31.405646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>here’s</td>\n",
       "      <td>28.096239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>nexa</td>\n",
       "      <td>26.410098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>vertigo</td>\n",
       "      <td>25.501868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>brazilians</td>\n",
       "      <td>23.437427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>world’s</td>\n",
       "      <td>22.315404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>hltv</td>\n",
       "      <td>20.056768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>devilwalk</td>\n",
       "      <td>19.948388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>alexi</td>\n",
       "      <td>19.675096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>plopski</td>\n",
       "      <td>19.64946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>diapers</td>\n",
       "      <td>18.626149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>karrigan</td>\n",
       "      <td>18.34275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>brazilian</td>\n",
       "      <td>17.980659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>robert</td>\n",
       "      <td>17.724467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>cadian</td>\n",
       "      <td>17.704199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>freya</td>\n",
       "      <td>17.693629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>aleksi</td>\n",
       "      <td>17.58855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word     weight\n",
       "3103     library  87.844072\n",
       "2514    xantares  43.479954\n",
       "2433   stockholm  37.353896\n",
       "2542    numerous  31.405646\n",
       "2915      here’s  28.096239\n",
       "1123        nexa  26.410098\n",
       "1814     vertigo  25.501868\n",
       "1766  brazilians  23.437427\n",
       "1598     world’s  22.315404\n",
       "646         hltv  20.056768\n",
       "804    devilwalk  19.948388\n",
       "2863       alexi  19.675096\n",
       "493      plopski   19.64946\n",
       "2035     diapers  18.626149\n",
       "179     karrigan   18.34275\n",
       "541    brazilian  17.980659\n",
       "1745      robert  17.724467\n",
       "1567      cadian  17.704199\n",
       "731        freya  17.693629\n",
       "512       aleksi   17.58855"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs.sort_values(by='weight', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dc93278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+\n",
      "|             message|   channel|pred_channel|\n",
      "+--------------------+----------+------------+\n",
      "|             hahahah|#asmongold|  #asmongold|\n",
      "|amber slaps girls...|#asmongold|  #asmongold|\n",
      "| HAHAHAHAHHAHAHAHAHA|#asmongold|  #asmongold|\n",
      "|     BROLLAN????????|      #pgl|        #pgl|\n",
      "|It’s cheaper to p...|#asmongold|  #asmongold|\n",
      "|                  no|#asmongold|  #asmongold|\n",
      "|     somebody farted|#asmongold|  #asmongold|\n",
      "|               Obama|#asmongold|  #asmongold|\n",
      "|depp stole amber'...|#asmongold|  #asmongold|\n",
      "|chr1st1an1337:  D...|      #pgl|        #pgl|\n",
      "+--------------------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select(['message', 'channel','pred_channel']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c88ea01a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8573022907468333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Make prediction\n",
    "predictionAndTarget = prediction.select(\"channel_bin\", \"prediction\")\n",
    "\n",
    "# Create evaluators\n",
    "metrics_binary = BinaryClassificationMetrics(predictionAndTarget.rdd.map(tuple))\n",
    "\n",
    "auc = metrics_binary.areaUnderROC\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c124183d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+\n",
      "|             message|   channel|pred_channel|\n",
      "+--------------------+----------+------------+\n",
      "|KappaPride IM JIG...|      #pgl|  #asmongold|\n",
      "|           gachiBASS|#asmongold|  #asmongold|\n",
      "|I only punched hi...|#asmongold|  #asmongold|\n",
      "|                KEKW|#asmongold|  #asmongold|\n",
      "|Yo I'd be gay for...|#asmongold|  #asmongold|\n",
      "|           YEBIII :D|      #pgl|  #asmongold|\n",
      "|          PauseChamp|#asmongold|  #asmongold|\n",
      "|   HAHAAHHAHAHHAHAAH|      #pgl|  #asmongold|\n",
      "|    ok sure dude lol|#asmongold|  #asmongold|\n",
      "|CANT HEAR ANY ING...|      #pgl|        #pgl|\n",
      "+--------------------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_raw = pd.read_csv(r\"data\\test.csv\", index_col=0).\\\n",
    "                        reset_index(drop=True)\n",
    "test_df = spark.createDataFrame(test_raw.astype(str))\n",
    "test_df = test_df.withColumn(\"message_clean\", clean_text(col(\"message\")))\n",
    "test_pred = model.transform(test_df)\n",
    "\n",
    "test_pred.select(['message', 'channel','pred_channel']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40e5db6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8520920705019877"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction\n",
    "predictionAndTarget = test_pred.select(\"channel_bin\", \"prediction\")\n",
    "\n",
    "# Create evaluator\n",
    "metrics_binary = BinaryClassificationMetrics(predictionAndTarget.rdd.map(tuple))\n",
    "\n",
    "#calculate auc for testing sample\n",
    "auc = metrics_binary.areaUnderROC\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccee482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"twitch_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae90f7b",
   "metadata": {},
   "source": [
    "## Predicting the stream data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "model = PipelineModel.load(\"twitch_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c5c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c2a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    df = df.withColumn(\"message_clean\", clean_text(col(\"message\")))\n",
    "    \n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = model # Replace '***' with:    [...].load('my_logistic_regression')\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model: \n",
    "    df_result = globals()['my_model'].transform(df)\n",
    "    df_result.select(['datetime', 'username','message', 'channel','pred_channel']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ac545",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 8080)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757942a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
